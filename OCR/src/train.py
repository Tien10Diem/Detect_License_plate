import os
import csv
import numpy as np
import tensorflow as tf
import keras
from keras import layers, models, callbacks

# --- 1. Cáº¤U HÃŒNH ---
IMG_WIDTH = 200
IMG_HEIGHT = 50
MAX_LABEL_LEN = 12
CHAR_LIST = "0123456789ABCDEFGHKLMNPSTUVXYZ-. JQORÄI" 

char_to_num = layers.StringLookup(vocabulary=list(CHAR_LIST), mask_token=None)
num_to_char = layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)
valid_chars = set(CHAR_LIST)

# --- 2. LOAD DATA ---
# ... (Pháº§n import vÃ  cáº¥u hÃ¬nh giá»¯ nguyÃªn) ...

# --- 2. LOAD DATA (Cáº¬P NHáº¬T Má»šI) ---

# HÃ m Ä‘á»c CSV cÅ© (Giá»¯ nguyÃªn)
def load_data_csv(img_folder, label_file):
    img_paths = []
    labels = []
    if not os.path.exists(label_file): return [], []
    with open(label_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) < 2 or row[0] == "Name": continue
            filename = row[0].strip()
            label = row[1].strip().upper()
            if any(c not in valid_chars for c in label): continue
            path = os.path.join(img_folder, filename)
            if os.path.exists(path):
                img_paths.append(path)
                labels.append(label)
    return img_paths, labels

# ğŸ”¥ HÃ€M Má»šI: Äá»c file TXT (Data tháº­t)
def load_data_txt(root_folder, label_file):
    img_paths = []
    labels = []
    if not os.path.exists(label_file): 
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file label: {label_file}")
        return [], []
    
    with open(label_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        for line in lines:
            parts = line.strip().split() # Tá»± Ä‘á»™ng tÃ¡ch theo khoáº£ng tráº¯ng hoáº·c Tab
            if len(parts) < 2: continue
            
            # Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n: ./train/abc.jpg -> train/abc.jpg
            rel_path = parts[0].replace('./', '').replace('/', os.sep)
            label = parts[1].strip().upper()
            
            # GhÃ©p Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§
            full_path = os.path.join(root_folder, rel_path)
            
            # Lá»c kÃ½ tá»± láº¡ (náº¿u cÃ³)
            if any(c not in valid_chars for c in label): continue
            
            if os.path.exists(full_path):
                img_paths.append(full_path)
                labels.append(label)
            else:
                # Debug nháº¹ náº¿u khÃ´ng tháº¥y áº£nh
                # print(f"âš ï¸ Thiáº¿u áº£nh: {full_path}") 
                pass
                
    return img_paths, labels

current_dir = os.path.dirname(os.path.abspath(__file__))
data_root = os.path.join(current_dir, '..', 'data')

# 1. Load Data CÅ© (Generated - Äá»ƒ model khÃ´ng quÃªn bÃ i cÅ©)
print("â³ Äang load data cÅ©...")
paths_old, labels_old = load_data_csv(os.path.join(data_root, 'generated'), os.path.join(data_root, 'label_generated.csv'))

# --- TÃŒM VÃ€ Sá»¬A ÄOáº N NÃ€Y TRONG FILE TRAIN.PY ---

# 2. Load Data Má»›i (Real - QUAN TRá»ŒNG)
# ThÆ° má»¥c gá»‘c (NÆ¡i chá»©a file txt vÃ  folder train)
new_data_folder = r"E:\Project_OCR\data" 

# ÄÆ°á»ng dáº«n chÃ­nh xÃ¡c tá»›i file label
new_label_file = r"E:\Project_OCR\data\rec_gt_train.txt"

print(f"â³ Äang load data Má»šI tá»«: {new_label_file}")
paths_new, labels_new = load_data_txt(new_data_folder, new_label_file)

# 3. Trá»™n láº¡i (Æ¯u tiÃªn data má»›i báº±ng cÃ¡ch nhÃ¢n báº£n nÃ³ lÃªn náº¿u nÃ³ Ã­t)
# Máº¹o: Náº¿u data tháº­t Ã­t (< 1000 áº£nh), ta nhÃ¢n Ä‘Ã´i nÃ³ lÃªn Ä‘á»ƒ Model há»c ká»¹ hÆ¡n
if len(paths_new) > 0:
    print(f"ğŸ”¥ TÃ¬m tháº¥y {len(paths_new)} áº£nh THáº¬T! (NhÃ¢n Ä‘Ã´i trá»ng sá»‘)")
    paths_new = paths_new * 2 # NhÃ¢n Ä‘Ã´i
    labels_new = labels_new * 2

img_paths = paths_old + paths_new
labels = labels_old + labels_new

# Shuffle ká»¹
combined = list(zip(img_paths, labels))
import random
random.shuffle(combined)
img_paths, labels = zip(*combined)
img_paths, labels = list(img_paths), list(labels)

print(f"âœ… Tá»”NG Cá»˜NG: {len(img_paths)} áº£nh (CÅ© + Má»›i).")

if len(img_paths) == 0: exit()

# ... (CÃ¡c pháº§n sau giá»¯ nguyÃªn) ...

# --- 3. PREPROCESSING ---
def encode_single_sample(img_path, label):
    img = tf.io.read_file(img_path)
    img = tf.io.decode_jpeg(img, channels=1)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
    # Táº¯t Augmentation Ä‘á»ƒ há»c á»•n Ä‘á»‹nh trong giai Ä‘oáº¡n cuá»‘i
    img = tf.transpose(img, perm=[1, 0, 2])
    label = char_to_num(tf.strings.unicode_split(label, input_encoding="UTF-8"))
    pad_len = MAX_LABEL_LEN - tf.shape(label)[0]
    label = tf.pad(label, [[0, pad_len]], constant_values=99)
    return {"image": img, "label": label}

BATCH_SIZE = 32
full_dataset = tf.data.Dataset.from_tensor_slices((img_paths, labels))
full_dataset = full_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)

split_idx = int(len(img_paths) * 0.9)
train_dataset = full_dataset.take(split_idx).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
validation_dataset = full_dataset.skip(split_idx).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# --- 4. MODEL & CALLBACKS ---
@keras.saving.register_keras_serializable()
class CTCLayer(layers.Layer):
    def __init__(self, name=None, **kwargs):
        super().__init__(name=name, **kwargs)
    def call(self, y_true, y_pred):
        batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
        input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64") * tf.ones(shape=(batch_len,), dtype="int64")
        label_length = tf.cast(tf.math.count_nonzero(tf.not_equal(y_true, 99), axis=1), dtype="int64")
        loss = tf.nn.ctc_loss(tf.cast(y_true, "int32"), y_pred, label_length, tf.cast(input_length, "int32"), logits_time_major=False, blank_index=-1)
        self.add_loss(tf.reduce_mean(loss))
        return y_pred

class MonitorCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"\nğŸ” Káº¿t quáº£ test Epoch {epoch+1}:")
        for batch in validation_dataset.take(1):
            imgs = batch['image']
            pred_model = models.Model(self.model.inputs[0], self.model.get_layer("dense_out").output)
            preds = pred_model.predict(imgs, verbose=0)
            input_len = np.ones(preds.shape[0]) * preds.shape[1]
            results = tf.keras.backend.ctc_decode(preds, input_length=input_len, greedy=True)[0][0]
            for i in range(min(3, len(results))):
                res_str = tf.strings.reduce_join(num_to_char(results[i])).numpy().decode("utf-8")
                print(f"   ğŸš— Biá»ƒn {i+1}: '{res_str}'")
            break

# --- 5. EXECUTION ---
print("ğŸ”„ Äang load model Ä‘á»ƒ TINH CHá»ˆNH (Fine-tune)...")
save_path = os.path.join(current_dir, '..', 'model', 'ocr_plate.keras')

if os.path.exists(save_path):
    model = models.load_model(save_path, custom_objects={'CTCLayer': CTCLayer})
    
    # ğŸ›‘ QUAN TRá»ŒNG: Chá»‰nh LR cá»±c nhá» (0.00005) Ä‘á»ƒ há»c chi tiáº¿t
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005)) 
    print("âœ… Cháº¿ Ä‘á»™: Precision Mode (LR=0.00005)")
else:
    print("âŒ ChÆ°a cÃ³ model cÅ©! HÃ£y cháº¡y train tá»« Ä‘áº§u trÆ°á»›c.")
    exit()

# Callback: Chá»‰ giáº£m LR khi thá»±c sá»± cáº§n thiáº¿t
lr_scheduler = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, verbose=1)
checkpoint = callbacks.ModelCheckpoint(save_path, monitor='loss', save_best_only=True, verbose=1)

print("ğŸš€ Báº®T Äáº¦U FINE-TUNING (MÃ i cho sáº¯c nÃ©t)...")
model.fit(train_dataset, validation_data=validation_dataset, epochs=20, callbacks=[MonitorCallback(), lr_scheduler, checkpoint])