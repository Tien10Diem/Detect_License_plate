import os
import cv2
import numpy as np
import tensorflow as tf
import keras
from keras import layers, models

# T·∫Øt log r√°c
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# --- C·∫§U H√åNH ---
IMG_WIDTH = 200
IMG_HEIGHT = 50
CHAR_LIST = "0123456789ABCDEFGHKLMNPSTUVXYZ-. JQORƒêI"

char_to_num = layers.StringLookup(vocabulary=list(CHAR_LIST), mask_token=None)
num_to_char = layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)
# --- 1. TH√äM H√ÄM S·ª¨A L·ªñI V√ÄO ƒê·∫¶U FILE EVALUATE.PY ---
def autocorrect_plate(text):
    text = text.replace('[UNK]', '').replace('_', '').replace('-', '').replace('.', '').strip()
    
    # T·ª´ ƒëi·ªÉn s·ª≠a l·ªói
    dict_char_to_num = {'O': '0', 'Q': '0', 'D': '0', 'I': '1', 'J': '3', 'L': '1', 'S': '5', 'B': '8', 'Z': '7', 'A': '4', 'G': '6'}
    dict_num_to_char = {'0': 'O', '1': 'I', '2': 'Z', '4': 'A', '5': 'S', '8': 'B', '7': 'T', '6': 'G'}

    chars = list(text)
    new_chars = []

    for i, c in enumerate(chars):
        # 2 k√Ω t·ª± ƒë·∫ßu l√† S·ªê (M√£ t·ªânh)
        if i < 2:
            new_chars.append(dict_char_to_num.get(c, c))
        # K√Ω t·ª± th·ª© 3 l√† CH·ªÆ (Series)
        elif i == 2:
            # N·∫øu l√† s·ªë th√¨ c·ªë √©p v·ªÅ ch·ªØ, kh√¥ng th√¨ gi·ªØ nguy√™n
            if c.isdigit():
                new_chars.append(dict_num_to_char.get(c, c))
            else:
                new_chars.append(c)
        # C√°c k√Ω t·ª± sau l√† S·ªê
        elif i > 2:
            new_chars.append(dict_char_to_num.get(c, c))
            
    return "".join(new_chars)

# --- 2. S·ª¨A ƒêO·∫†N V√íNG L·∫∂P TRONG H√ÄM evaluate ---
    # ...
    # for i in range(total):
        # ... (ƒëo·∫°n d·ª± ƒëo√°n gi·ªØ nguy√™n)
        # pred_text = decode_batch_predictions(probs)[0]
        
        # üî• TH√äM D√íNG N√ÄY: √Åp d·ª•ng s·ª≠a l·ªói tr∆∞·ªõc khi ch·∫•m ƒëi·ªÉm

# --- LOAD MODEL ---
print("‚è≥ ƒêang load model...")
model_path = os.path.join(os.path.dirname(__file__), '..', 'model', 'ocr_plate.keras')

if not os.path.exists(model_path):
    print("‚ùå Kh√¥ng t√¨m th·∫•y model!")
    exit()

@keras.saving.register_keras_serializable()
class CTCLayer(layers.Layer):
    def __init__(self, name=None, **kwargs): super().__init__(name=name, **kwargs)
    def call(self, y_true, y_pred): return y_pred

full_model = models.load_model(model_path, custom_objects={'CTCLayer': CTCLayer})
prediction_model = models.Model(inputs=full_model.inputs[0], outputs=full_model.get_layer("dense_out").output)
print("‚úÖ Load model th√†nh c√¥ng!")

# --- H√ÄM D·ª∞ ƒêO√ÅN ---
def decode_batch_predictions(pred):
    input_len = np.ones(pred.shape[0]) * pred.shape[1]
    # Greedy Search
    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]
    output_text = []
    for res in results:
        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode("utf-8")
        # L√†m s·∫°ch [UNK]
        res = res.replace('[UNK]', '').replace('_', '').strip()
        output_text.append(res)
    return output_text

# --- THAY TH·∫æ H√ÄM load_test_data C≈® B·∫∞NG H√ÄM N√ÄY ---
# --- THAY TH·∫æ H√ÄM load_test_data C≈® B·∫∞NG H√ÄM N√ÄY ---
def load_test_data(root_folder, label_file):
    img_paths = []
    labels = []
    
    if not os.path.exists(label_file): 
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file txt: {label_file}")
        return [], []
    
    with open(label_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 2: continue
            
            # --- ƒêO·∫†N S·ª¨A QUAN TR·ªåNG ---
            # Ch·ªâ l·∫•y ph·∫ßn T√äN FILE (b·ªè qua ./test/ ·ªü ƒë·∫±ng tr∆∞·ªõc)
            # V√≠ d·ª•: ./test/anh1.jpg -> anh1.jpg
            file_name = os.path.basename(parts[0]) 
            
            label = parts[1].strip().upper()
            
            # Gh√©p tr·ª±c ti·∫øp v√†o th∆∞ m·ª•c ch·ª©a ·∫£nh
            full_path = os.path.join(root_folder, file_name)
            
            if os.path.exists(full_path):
                img_paths.append(full_path)
                labels.append(label)
            else:
                # In ra l·ªói ƒë·∫ßu ti√™n ƒë·ªÉ d·ªÖ debug
                if len(img_paths) == 0:
                    print(f"‚ö†Ô∏è Th·ª≠ t√¨m: {full_path} -> KH√îNG TH·∫§Y!")
                    
    return img_paths, labels

# --- CH·∫†Y ƒê√ÅNH GI√Å ---
def evaluate(test_root, test_label):
    print(f"\nüìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu test t·ª´: {test_label}")
    paths, labels = load_test_data(test_root, test_label)
    
    total = len(paths)
    if total == 0:
        print("‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh test n√†o!")
        return

    print(f"‚ö° B·∫Øt ƒë·∫ßu ch·∫•m ƒëi·ªÉm tr√™n {total} ·∫£nh...")
    
    correct_count = 0
    error_cases = [] # L∆∞u l·∫°i c√°c ca sai ƒë·ªÉ soi

    # Duy·ªát t·ª´ng ·∫£nh (C√≥ th·ªÉ batch nh∆∞ng loop cho d·ªÖ debug)
    for i in range(total):
        # 1. X·ª≠ l√Ω ·∫£nh
        img = tf.io.read_file(paths[i])
        img = tf.io.decode_jpeg(img, channels=1)
        img = tf.image.convert_image_dtype(img, tf.float32)
        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
        img = tf.transpose(img, perm=[1, 0, 2])
        img = tf.expand_dims(img, axis=0)

        # 2. D·ª± ƒëo√°n
        preds = prediction_model.predict(img, verbose=0)
        probs = tf.nn.softmax(preds)
        pred_text = decode_batch_predictions(probs)[0]
        pred_text_corrected = autocorrect_plate(pred_text)
        
        ground_truth = labels[i].replace('-', '').replace('.', '') # L√†m s·∫°ch label g·ªëc lu√¥n cho c√¥ng b·∫±ng

        # So s√°nh c√°i ƒë√£ s·ª≠a
        if pred_text_corrected == ground_truth:
            correct_count += 1
        else:
            # In ra ƒë·ªÉ xem t·∫°i sao sai (quan tr·ªçng ƒë·ªÉ debug)
            error_cases.append((ground_truth, pred_text_corrected, paths[i]))
        ground_truth = labels[i]

        # 3. So s√°nh
        if pred_text == ground_truth:
            correct_count += 1
        else:
            error_cases.append((ground_truth, pred_text, paths[i]))
        
        # In ti·∫øn ƒë·ªô ki·ªÉu pro (m·ªói 10 ·∫£nh)
        if (i+1) % 100 == 0:
            print(f"   Processed {i+1}/{total}...")

    # --- K·∫æT QU·∫¢ ---
    accuracy = (correct_count / total) * 100
    print("\n" + "="*40)
    print(f"üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
    print("="*40)
    print(f"‚úÖ T·ªïng s·ªë ·∫£nh: {total}")
    print(f"üéØ S·ªë c√¢u ƒë√∫ng: {correct_count}")
    print(f"‚ùå S·ªë c√¢u sai:  {total - correct_count}")
    print(f"‚≠ê ƒê·ªò CH√çNH X√ÅC (ACCURACY): {accuracy:.2f}%")
    print("="*40)

    if len(error_cases) > 0:
        print("\nüíÄ DANH S√ÅCH C√ÅC C√ÇU SAI (Top 20):")
        print(f"{'TH·ª∞C T·∫æ':<15} | {'MODEL ƒêO√ÅN':<15} | {'FILE ·∫¢NH'}")
        print("-" * 60)
        for gt, pred, path in error_cases[:20]: # Ch·ªâ in 20 c√°i ƒë·∫ßu
            fname = os.path.basename(path)
            print(f"{gt:<15} | {pred:<15} | {fname}")
        
        # G·ª£i √Ω fix l·ªói
        print("\nüí° G·ª£i √Ω:")
        print("- N·∫øu sai k√Ω t·ª± gi·ªëng nhau (8-B, 0-O): C·∫ßn d√πng h√†m autocorrect.")
        print("- N·∫øu sai ho√†n to√†n: ·∫¢nh qu√° m·ªù ho·∫∑c nhi·ªÖu.")

if __name__ == "__main__":
    # 1. Tr·ªè th·∫≥ng v√†o folder ch·ª©a ·∫£nh
    test_data_folder = r"E:\Project_OCR\data\test_a" 
    
    # 2. Tr·ªè v√†o file txt
    test_label_file = r"E:\Project_OCR\data\rec_gt_test.txt" 

    evaluate(test_data_folder, test_label_file)